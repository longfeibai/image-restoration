digraph {
	graph [size="27.0,27.0"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2588467602640 [label="
 (1, 3, 64, 64)" fillcolor=darkolivegreen1]
	2588521894848 [label=ConvolutionBackward0]
	2588521894512 -> 2588521894848
	2588521894512 [label=AddBackward0]
	2588521896720 -> 2588521894512
	2588521896720 [label=UpsampleBilinear2DBackward0]
	2588521896960 -> 2588521896720
	2588521896960 [label=ReluBackward0]
	2588521897056 -> 2588521896960
	2588521897056 [label=NativeBatchNormBackward0]
	2588521897152 -> 2588521897056
	2588521897152 [label=ConvolutionBackward0]
	2588521897344 -> 2588521897152
	2588521897344 [label=AddBackward0]
	2588521897536 -> 2588521897344
	2588521897536 [label=UpsampleBilinear2DBackward0]
	2588521897680 -> 2588521897536
	2588521897680 [label=ReluBackward0]
	2588521897776 -> 2588521897680
	2588521897776 [label=NativeBatchNormBackward0]
	2588521897872 -> 2588521897776
	2588521897872 [label=ConvolutionBackward0]
	2588521898064 -> 2588521897872
	2588521898064 [label=AddBackward0]
	2588521898256 -> 2588521898064
	2588521898256 [label=UpsampleBilinear2DBackward0]
	2588521898400 -> 2588521898256
	2588521898400 [label=ReluBackward0]
	2588521898496 -> 2588521898400
	2588521898496 [label=NativeBatchNormBackward0]
	2588521898592 -> 2588521898496
	2588521898592 [label=ConvolutionBackward0]
	2588521898784 -> 2588521898592
	2588521898784 [label=ReluBackward0]
	2588521898976 -> 2588521898784
	2588521898976 [label=NativeBatchNormBackward0]
	2588521899072 -> 2588521898976
	2588521899072 [label=ConvolutionBackward0]
	2588521898208 -> 2588521899072
	2588521898208 [label=ReluBackward0]
	2588521899408 -> 2588521898208
	2588521899408 [label=NativeBatchNormBackward0]
	2588521899504 -> 2588521899408
	2588521899504 [label=ConvolutionBackward0]
	2588521897488 -> 2588521899504
	2588521897488 [label=ReluBackward0]
	2588521899840 -> 2588521897488
	2588521899840 [label=NativeBatchNormBackward0]
	2588521899936 -> 2588521899840
	2588521899936 [label=ConvolutionBackward0]
	2588521894896 -> 2588521899936
	2588521894896 [label=ReluBackward0]
	2588521900272 -> 2588521894896
	2588521900272 [label=NativeBatchNormBackward0]
	2588521900368 -> 2588521900272
	2588521900368 [label=ConvolutionBackward0]
	2588521900560 -> 2588521900368
	2588467522800 [label="encoder1.0.weight
 (64, 4, 4, 4)" fillcolor=lightblue]
	2588467522800 -> 2588521900560
	2588521900560 [label=AccumulateGrad]
	2588521900512 -> 2588521900368
	2588467522400 [label="encoder1.0.bias
 (64)" fillcolor=lightblue]
	2588467522400 -> 2588521900512
	2588521900512 [label=AccumulateGrad]
	2588521900320 -> 2588521900272
	2588467522320 [label="encoder1.1.weight
 (64)" fillcolor=lightblue]
	2588467522320 -> 2588521900320
	2588521900320 [label=AccumulateGrad]
	2588521900176 -> 2588521900272
	2588467522480 [label="encoder1.1.bias
 (64)" fillcolor=lightblue]
	2588467522480 -> 2588521900176
	2588521900176 [label=AccumulateGrad]
	2588521900128 -> 2588521899936
	2588467521840 [label="encoder2.0.weight
 (128, 64, 4, 4)" fillcolor=lightblue]
	2588467521840 -> 2588521900128
	2588521900128 [label=AccumulateGrad]
	2588521900080 -> 2588521899936
	2588467521680 [label="encoder2.0.bias
 (128)" fillcolor=lightblue]
	2588467521680 -> 2588521900080
	2588521900080 [label=AccumulateGrad]
	2588521899888 -> 2588521899840
	2588467521600 [label="encoder2.1.weight
 (128)" fillcolor=lightblue]
	2588467521600 -> 2588521899888
	2588521899888 [label=AccumulateGrad]
	2588521899744 -> 2588521899840
	2588467521440 [label="encoder2.1.bias
 (128)" fillcolor=lightblue]
	2588467521440 -> 2588521899744
	2588521899744 [label=AccumulateGrad]
	2588521899696 -> 2588521899504
	2588467517120 [label="encoder3.0.weight
 (256, 128, 4, 4)" fillcolor=lightblue]
	2588467517120 -> 2588521899696
	2588521899696 [label=AccumulateGrad]
	2588521899648 -> 2588521899504
	2588467517040 [label="encoder3.0.bias
 (256)" fillcolor=lightblue]
	2588467517040 -> 2588521899648
	2588521899648 [label=AccumulateGrad]
	2588521899456 -> 2588521899408
	2588467527520 [label="encoder3.1.weight
 (256)" fillcolor=lightblue]
	2588467527520 -> 2588521899456
	2588521899456 [label=AccumulateGrad]
	2588521899312 -> 2588521899408
	2588467527440 [label="encoder3.1.bias
 (256)" fillcolor=lightblue]
	2588467527440 -> 2588521899312
	2588521899312 [label=AccumulateGrad]
	2588521899264 -> 2588521899072
	2588467604640 [label="bottleneck.0.weight
 (512, 256, 4, 4)" fillcolor=lightblue]
	2588467604640 -> 2588521899264
	2588521899264 [label=AccumulateGrad]
	2588521899216 -> 2588521899072
	2588467598480 [label="bottleneck.0.bias
 (512)" fillcolor=lightblue]
	2588467598480 -> 2588521899216
	2588521899216 [label=AccumulateGrad]
	2588521899024 -> 2588521898976
	2588467598400 [label="bottleneck.1.weight
 (512)" fillcolor=lightblue]
	2588467598400 -> 2588521899024
	2588521899024 [label=AccumulateGrad]
	2588521898880 -> 2588521898976
	2588467604560 [label="bottleneck.1.bias
 (512)" fillcolor=lightblue]
	2588467604560 -> 2588521898880
	2588521898880 [label=AccumulateGrad]
	2588521898736 -> 2588521898592
	2588467598080 [label="decoder3.0.weight
 (512, 256, 4, 4)" fillcolor=lightblue]
	2588467598080 -> 2588521898736
	2588521898736 [label=AccumulateGrad]
	2588521898688 -> 2588521898592
	2588467598000 [label="decoder3.0.bias
 (256)" fillcolor=lightblue]
	2588467598000 -> 2588521898688
	2588521898688 [label=AccumulateGrad]
	2588521898544 -> 2588521898496
	2588467604240 [label="decoder3.1.weight
 (256)" fillcolor=lightblue]
	2588467604240 -> 2588521898544
	2588521898544 [label=AccumulateGrad]
	2588521898304 -> 2588521898496
	2588467604160 [label="decoder3.1.bias
 (256)" fillcolor=lightblue]
	2588467604160 -> 2588521898304
	2588521898304 [label=AccumulateGrad]
	2588521898208 -> 2588521898064
	2588521898016 -> 2588521897872
	2588467597680 [label="decoder2.0.weight
 (256, 128, 4, 4)" fillcolor=lightblue]
	2588467597680 -> 2588521898016
	2588521898016 [label=AccumulateGrad]
	2588521897968 -> 2588521897872
	2588467603920 [label="decoder2.0.bias
 (128)" fillcolor=lightblue]
	2588467603920 -> 2588521897968
	2588521897968 [label=AccumulateGrad]
	2588521897824 -> 2588521897776
	2588467603840 [label="decoder2.1.weight
 (128)" fillcolor=lightblue]
	2588467603840 -> 2588521897824
	2588521897824 [label=AccumulateGrad]
	2588521897584 -> 2588521897776
	2588467597600 [label="decoder2.1.bias
 (128)" fillcolor=lightblue]
	2588467597600 -> 2588521897584
	2588521897584 [label=AccumulateGrad]
	2588521897488 -> 2588521897344
	2588521897296 -> 2588521897152
	2588467603440 [label="decoder1.0.weight
 (128, 64, 4, 4)" fillcolor=lightblue]
	2588467603440 -> 2588521897296
	2588521897296 [label=AccumulateGrad]
	2588521897248 -> 2588521897152
	2588467597120 [label="decoder1.0.bias
 (64)" fillcolor=lightblue]
	2588467597120 -> 2588521897248
	2588521897248 [label=AccumulateGrad]
	2588521897104 -> 2588521897056
	2588467597040 [label="decoder1.1.weight
 (64)" fillcolor=lightblue]
	2588467597040 -> 2588521897104
	2588521897104 [label=AccumulateGrad]
	2588521896864 -> 2588521897056
	2588467603600 [label="decoder1.1.bias
 (64)" fillcolor=lightblue]
	2588467603600 -> 2588521896864
	2588521896864 [label=AccumulateGrad]
	2588521894896 -> 2588521894512
	2588521896624 -> 2588521894848
	2588467596960 [label="final.weight
 (3, 64, 1, 1)" fillcolor=lightblue]
	2588467596960 -> 2588521896624
	2588521896624 [label=AccumulateGrad]
	2588521896576 -> 2588521894848
	2588467596880 [label="final.bias
 (3)" fillcolor=lightblue]
	2588467596880 -> 2588521896576
	2588521896576 [label=AccumulateGrad]
	2588521894848 -> 2588467602640
}
